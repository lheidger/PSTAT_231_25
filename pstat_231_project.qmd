---
title: "231 Project"
author: "Lily Heidger"
format: 
  html:
    code-fold: show
    toc: true
    embed-resources: true
editor: visual
theme: lumen
execute:
  echo: true
  message: false
  warning: false
---

```{r}
.libPaths("~/rlibs")
library(osmdata)
library(here)
library(dplyr)
library(janitor)
library(sf)
library(mapview)
library(terra)
library(sf)
library(ggplot2)
library(tidycensus)
library(tidyverse)
library(knitr)
library(kableExtra)
library(tidymodels)
library(parsnip)
library(yardstick)
library(rsample)
library(nnet)
library(kknn)
```

# Can Urban Tree Canopy Predict Bike Infrastructure?

![\*Fig 1. A bike lane and sidewalk lined with trees (Credit: <https://x.com/stewart/status/599780176667357184?lang=ar>)\*](tree2.jpeg){width="666"}

# Introduction

Previous literature in urban studies and environmental justice has identified a correlation between areas with lower income individuals, lower education levels, or higher percentages of minority populations and overall lower urban tree canopy rates ([American Forests](https://www.americanforests.org/article/the-need-for-tree-equity-is-heating-up/)). Additionally, many of these same populations also have higher percentages of households or individuals without cars, meaning residents rely on biking, walking, and public transit ([Klein & Smart, 2019](https://www.jstor.org/stable/26911275?seq=1)). However, many lower-income neighborhoods lack the bike infrastructure to support the population that relies on biking. This project investigates the relationship between tree canopy and various socioeconomic variables, exploring whether a model of these variables can accurately predict the bike infrastructure on individual street segments.

For the purposes of this project, bike infrastructure is ranked in the following manner: no bike infrastructure (1), lane shared with cars AKA "sharrow" (2), painted bike lane (3), buffered or protected bike lane (4), and separated bike path (5). These rankings are originally based on the Canadian Bikeway Comfort and Safety (Can-BICS) classification system, shown in **Figure 2** ([Winters et al., 2020](https://www.canada.ca/en/public-health/services/reports-publications/health-promotion-chronic-disease-prevention-canada-research-policy-practice/vol-40-no-9-2020/canbics-classification-system-naming-convention-cycling-infrastructure.html)).

![\*Fig 2. Can-BICS classification system. (Credit: https://chatrlab.ca/projects/can-bics-english/)\*](canbics.png)

## Study Area: San Jose, California, USA

![\*Fig 3. San Jose, California.(Credit: https://climatecheck.com/california/san-jose)\*](sanjose.png)

San Jose is a city located south of the San Francisco Bay in Northern California. It spans 178 square miles and hosts about 970,000 residents. Over 38% of residents identify as Asian, 31% Hispanic or Latino, 29% White, and 3% Black. Nearly 8% of residents fall below the poverty level (US Census, 2023).

## Data

### Tree Canopy data

![\*Fig 4. US Forest Service Tree Canopy Data. (Credit: https://climatecheck.com/california/san-jose)\*](tree_data_pic.png)

The primary predictor and focus of this research question involves tree canopy cover data from the US Forest service, found [here](https://usfs.hub.arcgis.com/maps/dc0b6780a9ea4b8ea88057a3b26f7025/explore?location=34.419949%2C-119.277200%2C5.66). These data are pre-processed remote sensing TIF files indicating where tree canopy cover is found. The data were most recently updated in 2021.

**Citation:** US Forest Service. “Urban Tree Canopy in California.” Accessed April 12, 2025. <https://usfs.hub.arcgis.com/maps/dc0b6780a9ea4b8ea88057a3b26f7025/explore?location=34.419949,-119.277200,5.66>.

### Census variable data

The remaining predictors in this project are sourced from the 2020 Census, found [here](https://www.census.gov/programs-surveys/decennial-census/decade/2020/2020-census-results.html). These Census data are block group scale, with the selected variables as follows:

-   Median household income

-   \% high school educated

-   \% college educated

-   \% who commute via bicycle

-   \% who commute via bicycle who are male

-   \% who identify as Black

-   \% who identify as Hispanic

-   \% below the poverty level

-   \% who are veterans

**Citation:** U.S. Census Bureau. "2020 American Community Survey 5-Year Estimates," 2020, \<http://api.census.gov/data/2022/acs/acs5\>, accessed on April 12, 2025.

### OpenStreetMap data

![\*Fig 5. OpenStreetMap data interface. (Credit: https://wiki.openstreetmap.org/wiki/Map_Data_layer)\*](osm_pic.png){width="711"}

OSM data are open-source urban infrastructure and point-of-interest (POI) data, publicly available [here](https://www.openstreetmap.org/#map=4/-39.27/-73.60) and accessed in this project with the `osmdata` API. For this project, bike infrastructure data are accessed and cleaned, ranking the categorical infrastructure types in an ordinal and numerical column called comfort_level:

-   “Separate” (AKA separated bike or multiuse path) = 5

-   “Track” (AKA buffered or protected bike lane) = 4

-   “Lane” (AKA painted bike lane) = 3

-   “Shared_lane” (AKA “sharrow” or shared roadway) = 2

-   “None” (AKA no bike infrastructure) = 1

**Citation:** OpenStreetMap contributors (2024). OpenStreetMap \[CycleOSM\]. OpenStreetMap Foundation. Availabl as open data under the Open Data Commons Open Database License (ODbl) at openstreetmap.org. Accessed April 12, 2025.

## Research Questions

With these data in mind, our research questions are as follows:

-   Do areas with higher urban canopy percentages also have more bike infrastructure?

-   Can urban canopy serve as a reliable predictor of bike infrastructure at the street level?

## EDA

My methods first involved exploring the data and the relationship of several variables. First, I read in the tree canopy data and OSM data, exploring the distribution of observations in the various bike infrastructure categories. Missing data was not an issue with this dataset, so no action was taken to handle missingness.

### Tree Canopy Data

```{r, eval = FALSE}
sj_boundary <- st_read(here("PSTAT_231_25", "data", "san_jose", "urbanboundary", "San_Jose.shp"))
sf_boundary <- st_read(here("PSTAT_231_25","data", "San_Francisco_and_Oakland", "urbanboundary", "San_Francisco_and_Oakland.shp"))

sj_canopy <- rast(here("PSTAT_231_25", "data", "san_jose", "urbancanopy2022", "San_Jose_canopy2022.tif"))
colnames(sj_canopy)

saveRDS(sj_boundary, here("data", "sj_boundary.rds"))
saveRDS(sf_boundary, here("data", "sf_boundary.rds"))


```

### Osm Data

*(ChatGPT used to learn how to use `osmdata`)*

```{r}
sj_boundary <- readRDS(here("data", "sj_boundary.rds"))
sf_boundary <- readRDS(here("data", "sf_boundary.rds"))

san_jose <- getbb("San Jose, California")

all_streets <- san_jose %>%
  opq() %>%
  add_osm_feature(key = "highway", 
                  value = c("motorway", "trunk", "primary", "secondary", 
                            "tertiary", "residential", "unclassified", 
                            "service", "living_street")) %>%
  osmdata_sf()

streets_sf <- all_streets$osm_lines
```

```{r}
streets_clean <- streets_sf %>%
  select(osm_id, name, highway, cycleway, `cycleway:left`, `cycleway:right`, 
         bicycle, maxspeed, geometry) %>%
  mutate(
    bike_infra = case_when(
      # Bike path
      cycleway == "separate" ~ "bike path",
      
      # Buffered lane
      cycleway == "track" | `cycleway:left` == "track" ~ "buffered lane",
      
      # Lane
      cycleway == "lane" | 
        `cycleway:left` == "lane" | 
        `cycleway:right` == "lane" | 
        `cycleway:right` == "lane" ~ "lane",
      
      # Sharrow
      cycleway == "shared_lane" |
        `cycleway:left` == "shared_lane" |
        `cycleway:right` == "shared_lane" ~ "sharrow",
      
      # None
      cycleway %in% c("no", "opposite_lane") |
        `cycleway:left` == "no" |
        `cycleway:right` == "no" ~ "none",
      
      # Default fallback
      TRUE ~ "none"
    )
  )


ggplot(streets_clean, aes(bike_infra))+
  geom_bar() +
  ggtitle("San Jose Bike Infrastructure by Category") +
  theme_minimal()


# ggplot() +
#   geom_sf(data = streets_clean, aes(color = bike_infra), size = 0.8) +
#   scale_color_brewer(palette = "Set1") +
#   theme_minimal() +
#   labs(title = "San Jose Streets with Bike Infrastructure", 
#        color = "Bike Infrastructure Type")

streets_clean %>%
  count(bike_infra) %>%
  arrange(desc(n))


streets_clean <- streets_clean %>%
  mutate(
    maxspeed = as.numeric(gsub("[^0-9.]", "", maxspeed)) 
  ) %>%
  filter(maxspeed <= 60 | is.na(maxspeed)) %>% select(-`cycleway:left`, -`cycleway:right`)

#st_write(streets_clean, "5san_jose_streets_clean.gpkg")

```

Figure 5. Number of different segments of each bike infrastructure category in San Jose.

This bar plot gives us a good sense of the overall frequency of each of the different types of bike infrastructure across San Jose. A majority of streets are marked as "none", meaning they have no bike infrastructure, with streets with painted bike lanes being second. "Sharrow" or shared roads are third most common, followed by bike paths, then buffered lanes. This means that our model should have a higher probability of predicting street segments with no bike infrastructure. I can also infer that I need to stratify our sampling techniques by bike infrastructure to make sure I have a similar distribution in each of our datasets and consider upsampling or downsampling to even out the differences.

### Read in clean tree and bike data

After loading and cleaning the initial tree canopy data and OSM data, I saved both and uploaded them to ArcGIS Pro for further data wrangling. In ArcGIS, I first buffered each of the street segments by 25 meters to capture and evaluate the street trees surrounding each street segment. Then, because the tree canopy data are raster data, I used the zonal statistics tool to analyze and assign the number of tree canopy pixels within each buffer and assign that value to the subsequent segment. I then calculated the area in $m^2$ of each segment and divided the pixel value by the area to calculate percent canopy coverage for each segment. I then uploaded the dataset back into R.

```{r}
sj_buffer <- st_read(here("data", "City_Limits", "main_Buffer4_Clip.shp"))

sj_city_bound <- st_read(here("data", "City_Limits.shp"))
# Map canopy density on each buffered street segment
#mapview(sj_buffer["canopy_den"])

saveRDS(sj_buffer, here("data", "sj_buffer.rds"))
sj_buffer <- readRDS(here("data", "sj_buffer.rds"))

```

San Jose City Limits accessed from https://gisdata-csj.opendata.arcgis.com/search?tags=boundaries

### Box plots

Now we'll explore some basic box plots to identify any correlation between canopy density and bike infrastructure. First, I have to make bike infrastructure an ordinal, numeric field.

```{r}
sj_buffer_clean <- sj_buffer %>% mutate(
    comfort_level = case_when(
      bike_infra == "bike path" ~ "5",

      bike_infra == "buffered lane" ~ "4",
      
      bike_infra == "lane" ~ "3",
      
      bike_infra == "sharrow" ~ "2",
      
      bike_infra == "none" ~ "1"))

as.numeric(sj_buffer_clean$comfort_level)


ggplot(sj_buffer_clean, aes(x = factor(comfort_level), y = canopy_den)) +
  geom_boxplot(fill = "lightblue") +
  labs(
    x = "Bike Infrastructure",
    y = "Tree Canopy Density",
    title = "Canopy Density by Bike Infrastructure Comfort Level"
  ) +
  theme_minimal()

```

Figure 6. Box plots of canopy density distribution for each comfort level.

This figure explores the basic relationship between tree canopy density and bike infrastructure, looking for any trends or differences among groups. Bike infrastructure level 3, segments with painted bike lanes, appears to have the lowest median tree canopy density of all of the groups. On the contrary, bike infrastructure level 5, segments with separated bike paths, has the highest median tree canopy density. These differences in tree canopy density among groups may indicate the potential for using tree canopy as a predictor for bike infrastructure.

## Census data

I then access US Census data using the `get_acs` API. This API pulls American Community Survey data straight from the Census website. I then clean the data and calculate percentages for each of the variables.

```{r}
options(tigris_use_cache = TRUE)

sj_blocks <- get_decennial(
  geography = "block group",
  variables = "P1_001N",  # Total population
  state = "CA",
  county = "Santa Clara",
  year = 2020,
  geometry = TRUE
)

#st_write(sj_blocks, "santa_clara_census_blocks.gpkg")

sj_city_bound <- st_transform(sj_city_bound, 4326)
sj_blocks <- st_transform(sj_blocks, 4326)


sj_blocks_clip <- st_intersection(sj_blocks, sj_city_bound)
```

```{r, eval = FALSE}
set.seed(123)

sj_sample <- sj_buffer_clean %>%
  dplyr::slice_sample(n = 10000)

sj_sample <- st_transform(sj_sample, 4326)


#ACS data
ca_tracts <- get_acs(
  geography = "tract",
  variables = c(
    "B19013_001",   # Median household income
    "B15003_017",   # College education
    "B15003_022",   # High school graduate
    "B08301_001",   # Total commuters
    "B08301_019",   # Bicycle commuters
    "B02001_003",   # Black or African American
    "B01003_001",   # Total population
    "B03003_003",   # Hispanic or Latino
    "B17001_002",   # Below poverty level
    "B21001_002"    # Veterans
  ),
  state = "CA",
  county = "Santa Clara",
  year = 2020,
  geometry = TRUE
)

#pivot wider
ca_tracts <- ca_tracts %>%
  select(GEOID, NAME, variable, estimate, geometry) %>%
  pivot_wider(
    names_from = variable,
    values_from = estimate
  )

#calculate percentages
tract_data <- ca_tracts %>%
  st_drop_geometry() %>%
  rename(
    income_med = B19013_001,
    educ_college = B15003_017,
    educ_high_school = B15003_022,
    commuters_total = B08301_001,
    bicycle_commuters = B08301_019,
    pop_black = B02001_003,
    pop_total = B01003_001,
    pop_hispanic = B03003_003,
    pop_poverty = B17001_002,
    pop_veteran = B21001_002
  ) %>%
  mutate(
    educ_high_school_perc = educ_high_school / pop_total,
    educ_college_perc = educ_college / pop_total,
    bicycle_perc = bicycle_commuters / commuters_total,
    pct_black = pop_black / pop_total,
    pct_hispanic = pop_hispanic / pop_total,
    pct_poverty = pop_poverty / pop_total,
    pct_veteran = pop_veteran / pop_total
  ) %>%
  left_join(
    ca_tracts %>% select(GEOID, geometry),
    by = "GEOID"
  ) %>%
  st_as_sf()

tract_data <- st_transform(tract_data, st_crs(streets_buffers))


#interpolation
streets_with_demographics <- cbind(
  sj_sample %>% 
    dplyr::select(osm_id, name),
  
  st_interpolate_aw(
    tract_data[c(
      "income_med",
      "educ_high_school_perc",
      "educ_college_perc",
      "bicycle_perc",
      "pct_black",
      "pct_hispanic",
      "pct_poverty",
      "pct_veteran"
    )],
    streets_buffers,
    extensive = FALSE,  # FALSE for percentages/rates
    keep_NA = TRUE
  ) %>%
    st_drop_geometry()
)
saveRDS(tract_data, here("data", "tract_data.rds"))
saveRDS(streets_with_demographics, here("data", "streets_with_demographics.rds"))

```

```{r}
tract_data <- readRDS(here("data", "tract_data.rds"))
streets_with_demographics <- readRDS(here("data", "streets_with_demographics.rds"))

ggplot(tract_data) +
  geom_sf(aes(fill = income_med), color = NA) +
  scale_fill_viridis_c(option = "plasma") +  # nice gradient for numeric values
  labs(title = "Median Income by Census Tract", fill = "Income") +
  theme_minimal()
```


This map provides a good exploration of the spatial distribution of median household income at the tract level in San Jose. Lower income residents appear to be clustered in the downtown core of the city, with median income increasing as one moves away from the downtown area. These interesting patterns may be useful in our model.

*\[ChatGPT used to determine how to randomly sample my original sf object due to computational limitations.\]*

## Merging Datasets

Next, I merge data into one dataset to use in our models. I join the street segment OSM and tree canopy data to the Census data by the column "osm_id."

```{r}
sj_buffer_clean <- st_transform(sj_buffer_clean, 4326)

sj_buffer <- st_make_valid(sj_buffer)
sj_blocks <- st_make_valid(sj_blocks)

streets_blocks_intersection <- st_intersection(sj_buffer, sj_blocks)

#length of each street segment within each block
streets_blocks_intersection$segment_length <- st_length(streets_blocks_intersection)

streets_population <- streets_blocks_intersection %>%
  group_by(osm_id) %>%
  summarize(
    total_length = sum(segment_length),
    weighted_population = sum(P1_001N * as.numeric(segment_length) / total_length),
    population_density = weighted_population / total_length
  )

streets_with_population <- streets_clean %>%
  left_join(streets_population, by = "osm_id")

osm_census <- left_join(
  sj_buffer_clean,
  st_drop_geometry(streets_with_demographics),
  by = "osm_id"
)

osm_census_clean <- osm_census %>% select(-bicycle, -cycleway, -bike_infra, -BUFF_DIST, -ORIG_FID, -ZONE_CODE, -COUNT, -AREA, -SUM, -area_m2, -geom_Lengt, -geom_Area, -name.y, -geometry, -osm_id, -name.x, -maxspeed, -highway) %>% drop_na() %>% st_drop_geometry() %>%  filter(comfort_level != 4)

osm_census_clean_sf <- osm_census %>% select(-bicycle, -cycleway, -bike_infra, -BUFF_DIST, -ORIG_FID, -ZONE_CODE, -COUNT, -AREA, -SUM, -area_m2, -geom_Lengt, -geom_Area, -name.y, -geometry, -osm_id, -name.x, -maxspeed, -highway) %>% drop_na() %>%  filter(comfort_level != 4)


saveRDS(osm_census_clean, here("osm_census_clean.rds"))

```

*\[[Claude LLM](https://claude.ai/) used to help build code for accurately assigning population weighting to street segments\]*

### Multicollinearity

In order to avoid collinearity among my predictors, I created two correlation matrices: the first with all of the predictors, and the second after removing predictors to ensure I've removed the correct variables to avoid collinearity.

```{r}
library(ggcorrplot)
library(lares)
numeric_cols <- osm_census_clean %>% select(-comfort_level)

corr <- round(cor(na.omit(numeric_cols)), 1)
corr[is.na(corr) | is.nan(corr) | is.infinite(corr)] <- 0

ggcorrplot(corr, 
            hc.order = TRUE, 
            type = "lower", 
            lab = TRUE, 
            outline.color = "white")

correlations <- corr_cross(df = numeric_cols %>%
             data.frame(), # name of dataset
             top = 13)#,
             #top = 20)

correlations
```

The top few bars in this plot indicate that several predictors have a correlation coefficient above 0.7. In order to most efficiently avoid correlation, I remove `pct_hispanic` and `educ_college_perc`.

```{r}
data_clean <- numeric_cols %>% select(-pct_hispanic, -educ_college_perc)
correlations2 <- corr_cross(df = data_clean %>%
             data.frame(), 
             top = 13)

correlations2
```

Removing these two predictors eliminated the main correlation issues, verified in the plot above.

# Data Splitting and Cross-Validation

To prep our data for future modeling, I split our initial dataset into several different groups. The first splits the data into 75% for training the model and 25% for testing the model, using a method called stratification that ensures that the subsets of data are representative of the original dataset. For this project, I stratify based on comfort level, making sure that the subset datasets have the same proportion of comfort level classifications as the original.

After splitting into training and testing, I also created 5 folds or groups of data within our training dataset. I again stratify based on comfort level. These 5 folds will be used for cross-validation, a method that allows us to test our model without compromising our testing dataset. After fitting my data to the models, I will cross-validate it with these 5 folds of training data, where five separate times, the model is trained on 4 of the folds of data and then tested on the remaining fold. This then offers us an idea of how well the model performs.

```{r}
data_clean2 <- readRDS(here("osm_census_clean.rds")) %>% select(-pct_hispanic, -educ_college_perc) 
osm_census_split <- initial_split(data_clean2, prop = 0.75, strata = comfort_level)

osm_census_train <- training(osm_census_split)
osm_census_test <- testing(osm_census_split)

osm_census_train$comfort_level <- as.factor(osm_census_train$comfort_level)
osm_census_test$comfort_level <- as.factor(osm_census_test$comfort_level)

folds <- vfold_cv(osm_census_train, v = 5, strata = comfort_level) 
```

# Model Fitting

In this section, I fit multiple models to the training data: multinomial regression, k-nearest neighbors, elastic net, and random forest. Due to the extreme imbalance in the number of observations for each comfort level, with comfort level 1 having significantly more observations, I decided to upsample based on comfort level. I chose an over_ratio of 0.5 so that the minority comfort levels will have about half of the number of observations as comfort level 1.

## Recipe

```{r}
library(themis)

recipe <- recipe(comfort_level ~ ., data = osm_census_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_upsample(comfort_level, over_ratio = 0.5)
```

## Multinomial Regression Model

The following section creates and evaluates a multinomial regression model.

```{r}
mn_m1 <- multinom_reg(penalty = 0) %>%
  set_engine("glmnet") %>%
  set_mode("classification")


mn_wflow <- workflow() %>% 
  add_model(mn_m1) %>% 
  add_recipe(recipe)

mn_fit <- mn_wflow %>% 
  fit(data = osm_census_train)

mn_fit %>%
  extract_fit_parsnip() %>%
  tidy(exponentiate = FALSE)

# Prediction
pred_mn <- predict(mn_fit, osm_census_train, type = "prob") %>% bind_cols(osm_census_train)

pred_mn <- pred_mn %>%
  mutate(predicted_class = colnames(select(., starts_with(".pred_")))[
    max.col(select(., starts_with(".pred_")), ties.method = "first")
  ]) %>%
  mutate(predicted_class = as.integer(gsub(".pred_", "", predicted_class)))

pred_mn <- pred_mn %>%
  mutate(
    predicted_class = factor(predicted_class, levels = levels(comfort_level))
  )

accuracy(pred_mn, comfort_level, predicted_class)

```

The multinomial regression model shows an accuracy of 75% on the training data.

\[[*Tidymodels website*](https://www.tidymodels.org/start/resampling/) *used to understand resampling for validation. ChatGPT used to troubleshoot.\]*

## KNN Model

The section below creates several k-nearest neighbor models, selecting the best value of k for accuracy.

```{r}
library(themis)
library(vip)
knn_m1 <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")


knn_wflow <- workflow() %>% 
  add_model(knn_m1) %>% 
  add_recipe(recipe)
knn_grid <- grid_regular(neighbors(range = c(5, 10)), levels = 10)


knn_fit <- knn_wflow %>% 
  tune_grid(resamples = folds, grid = knn_grid)
knn_fit %>% collect_metrics()

knn_best <- select_best(knn_fit, metric = "accuracy")
final_knn_wflow <- finalize_workflow(knn_wflow, knn_best)

final_knn_fit <- fit(final_knn_wflow, data = osm_census_train)

pred_knn <- final_knn_fit %>% 
  predict(new_data = osm_census_train) %>%
  bind_cols(osm_census_train %>% select(comfort_level))

metrics(pred_knn, 
        truth = comfort_level, 
        estimate = .pred_class)

yardstick::conf_mat(data = pred_knn, truth = comfort_level, estimate = .pred_class)

prepped_recipe <- prep(recipe, training = osm_census_train)
baked_data <- bake(prepped_recipe, new_data = NULL)

X <- select(baked_data, -comfort_level)
y <- baked_data$comfort_level

vip(
  knn_model,
  method = "permute",
  train = X,
  target = y, 
  metric = "accuracy",
  pred_wrapper = function(object, newdata) predict(object, newdata = newdata, type = "raw")
)

```

The best knn model has 8 neighbors and shows an accuracy of 91% on the training data.

## Elastic Net Model

The following section creates and evaluates an elastic net model. Various hyperparameters are tuned and the best model is selected.

```{r}
en_m1 <- multinom_reg(
  mixture = tune(),
  penalty = tune()
) %>%
  set_mode("classification") %>%
  set_engine("glmnet")


en_wflow <- workflow() %>% 
  add_model(en_m1) %>% 
  add_recipe(recipe)
en_grid <- grid_regular(penalty(),
                     mixture(range = c(0, 1)), levels = 10)

en_fit <- en_wflow %>% 
  tune_grid(resamples = folds, grid = en_grid)
en_fit %>% collect_metrics()

en_best <- select_best(en_fit, metric = "accuracy")
final_en_wflow <- finalize_workflow(en_wflow, en_best)

final_en_fit <- fit(final_en_wflow, data = osm_census_train)

pred_en <- final_en_fit %>% 
  predict(new_data = osm_census_train) %>%
  bind_cols(osm_census_train %>% select(comfort_level))

# Calculate metrics
metrics(pred_en, 
        truth = comfort_level, 
        estimate = .pred_class)

yardstick::conf_mat(data = pred_en, truth = comfort_level, estimate = .pred_class)

vip(
  pred_en,
  method = "permute",
  train = X,
  target = y, 
  metric = "accuracy",
  pred_wrapper = function(object, newdata) predict(object, newdata = newdata, type = "raw")
)
```

The best elastic net model has a penalty of 1 and mixture of 0.111, showing an accuracy of 86% on the training data.

## Random Forest Model

The following section details the creation of a random forest model. The model is tuned using various hyperparameters, and the best performing model is selected.

```{r}
library(ranger)
rf_m1 <- rand_forest(mtry = tune(), 
                           trees = tune(), 
                           min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

rf_m1_wf <- workflow() %>% 
  add_model(rf_m1) %>% 
  add_recipe(recipe)

rf_grid <- grid_regular(mtry(range = c(1, 6)), 
                        trees(range = c(100, 400)),
                        min_n(range = c(10, 20)),
                        levels = 5)
```

```{r, eval = FALSE}
tune_rf <- tune_grid(
  rf_m1_wf,
  resamples = folds,
  grid = rf_grid
)
rf_best <- select_best(tune_rf, metric = "roc_auc")
```

```{r, eval = FALSE}
save(tune_rf, file = "tune_rf.rda")
```

```{r}
load("tune_rf.rda")

autoplot(tune_rf) +theme_minimal()

# Step 2: Finalize the workflow with those parameters
rf_final_wflow <- finalize_workflow(rf_m1_wf, rf_best)

# Step 3: Fit the final model on the full training set
rf_final_fit <- fit(rf_final_wflow, data = osm_census_train)  # or your training data

# Step 4: Extract the fitted model and plot variable importance
library(vip)
rf_final_fit %>%
  extract_fit_parsnip() %>%
  vip() +
  theme_minimal()

rf_preds <- predict(rf_final_fit, new_data = osm_census_train) %>%  
  bind_cols(osm_census_train %>% select(comfort_level))             

accuracy(rf_preds, truth = comfort_level, .pred_class)

augment(rf_final_fit, new_data = osm_census_train) %>% 
  conf_mat(truth = comfort_level, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

```

*\[Lab 7 used as a reference.\]*

# Model Selection and Performance

```{r}

pred_knn_test <- final_knn_fit %>% 
  predict(new_data = osm_census_test) %>%
  bind_cols(osm_census_test %>% select(comfort_level))

metrics(pred_knn_test, 
        truth = comfort_level, 
        estimate = .pred_class)

yardstick::conf_mat(data = pred_knn_test, truth = comfort_level, estimate = .pred_class)


accuracy(pred_knn_test, truth = comfort_level, estimate = .pred_class)
```

```{r}
accuracy(rf_pred_test, truth = comfort_level, .pred_class)

augment(rf_final_fit, new_data = osm_census_test) %>% 
  conf_mat(truth = comfort_level, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```

The best-performing model, chosen based on the accuracy both on the training set and then comparing model performance on the testing set, is the random forest model with an mtry of 1, 400 trees, and a min_n of 12. While this model has a high accuracy of 85% on the training data, this appears to be largely due to the large percent of the dataset made up by comfort level 1. The model even incorrectly classifies comfort level 3 as level 1 more often than any other level. This dataset is proving to be difficult for these models due to the large oversampling of comfort level 1.

# Conclusion

Of the four models evaluated, the random forest model performed the best, with an accuracy of 85% on the testing data. This model did an overall good job at predicting, but still tends to over-predict comfort level 1. The multinomial model had the lowest performance, with an accuracy of 75% on the training data. Overall, model performance was logical. The random forest has the flexibility to fit the data, overfitting a bit, yet still performing with higher accuracy. The multinomial regression model was not flexible enough for the data.

The random forest model ranked the predictor importance as follows: bicycle percent, percent poverty, canopy density, median household income, percent veterans, percent black, and percent high school educated. While this doesn't show that canopy density is the *most* important predictor, it still ranks high and proves to be useful. Ultimately, this project "reverse-engineers" the connection between sociodemographics and urban planning, nodding towards the claim that underserved communities are often less served by urban planning, where they often lack tree canopy *and* bicycle infrastructure. I hope to continue to explore this connection and evaluate whether tree canopy density is useful in other models such as predicting bicycle ridership on individual street segments.

Below I map the final correct and incorrect predictions from the random forest model. The error appears to be spatially random, but it would require a moran's I test to confirm.

```{r}
test_indices <- as.integer(rownames(osm_census_test))
osm_census_test_sf <- osm_census_clean_sf[test_indices, ]

rf_pred_test <- predict(rf_final_fit, new_data = osm_census_test, type = "class") %>%
  bind_cols(osm_census_test %>% select(comfort_level))

rf_pred_test <- rf_pred_test %>%
  mutate(correct = if_else(.pred_class == comfort_level, "yes", "no"))


osm_pred_sf <- osm_census_test_sf %>%
  mutate(
    predicted = rf_pred_test$.pred_class,
    actual = rf_pred_test$comfort_level,
    correct = rf_pred_test$correct
  )

ggplot(osm_pred_sf) +
  geom_sf(aes(fill = correct), color = NA) +
  scale_fill_manual(values = c("yes" = "forestgreen", "no" = "red")) +
  labs(title = "Prediction Accuracy by Location", fill = "Correct?") +
  theme_minimal()
```

*\[ChatGPT used to troubleshoot reattaching spatial geometries.\]*
